---
title: "NASA Engine Predictive Maintenance"
author: "Brian Baller"
date: "Summer 2021"
output:
  pdf_document:
    toc: yes
  html_document:
    number_sections: yes
    toc: yes
    fig_width: 7
    fig_height: 4.5
    theme: readable
    highlight: tango
---

TODO:   ROC curves; false pos v false neg; function for false negs...
        loop for optimal RUL for class 2 in training
        title and labels for ggplot
        remove outliers?
        Double-check predict syntax for classification
        Why does GAM underperform with FD001?
        Add table function for boost?
        Tune SVR


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

options(scipen=999)

library(glmnet)
library(gam)
library(e1071)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(earth)
options(java.parameters = "-Xmx12g")
library(rJava)
library(bartMachine)
library(caret)
library(dplyr)
```

```{r Data Import}
list.files()
train <- read.csv('input/train_FD001.txt', sep="", header=F)
test <- read.csv('input/test_FD001.txt', sep="", header=F)
```

```{r Name the Columns}
op_cols <- vector()
sens_cols <- vector()

for (i in 1:3) {
  op_cols[i] <- paste("op_set", i, sep="")
} 
for (i in 1:21) {
  sens_cols[i] <- paste("sensor", i, sep="")
} 

colnames(train)[1] <- 'unit'
colnames(train)[2] <- 'time'
colnames(train)[3:5] <- op_cols
colnames(train)[6:26] <- sens_cols

colnames(test)[1] <- 'unit'
colnames(test)[2] <- 'time'
colnames(test)[3:5] <- op_cols
colnames(test)[6:26] <- sens_cols
```

```{r Calculate the RUL for the Training Data}
#find the number of units in the training set
num_units <- max(train[,"unit"])

#find on which cycle each unit fails
cycle_fail <- vector()
for (i in 1:num_units) {
  cycle_fail[i] <- max(train[train$unit == i,"time"])
}

#add fail cycle to dataframe as a column
for (i in 1:num_units) {
  train$t_fail[train$unit == i] <- cycle_fail[i]
}

#add a column for RUL
train$RUL <- train$t_fail - train$time
```

```{r Response EDA}

## How do we handle this distro?  RUL^.6 closest to normal

hist(train$RUL)
hist(log(train$RUL))
hist(train$RUL^.6)
```


```{r Visualize Sensor Data}
## TODO: add-in titles and labels

#filter df down to just units 1-14 (for readability)
smalldf <- train[train$unit < 15,]

#plot sensor readings against RUL for 14 of the units
for (i in 3:26) {
  print(ggplot(smalldf, aes(-RUL, smalldf[,i], group = unit, color = factor(unit))) +
    geom_point())
}
```

```{r Barplots and Scatterplots of Data}
#summary(df)

## explore data 
par(mfrow=c(2,3))
for (i in 1:ncol(train)) {
  barplot(table(train[,i]), main=colnames(train)[i])
  #hist(train[,i], main=colnames(train)[i])
}

for (i in 1:ncol(train)) {
  plot(RUL ~ train[,i], train, main=colnames(train)[i], xlab="")
}
```


```{r Remove Unnecessary Vars}
# several sensor readings readings are constant regardless of RUL
if (ncol(train) == 28) {
  train <- subset(train, select = -c( unit,  time , op_set3, sensor1, sensor5, sensor10, sensor16, sensor18, sensor19, t_fail))
}
```

## Models

### Exploratory Linear Model

```{r Initial Linear Model}
## no xform of RUL
lm.mod <- lm(RUL ~ ., train)
summary(lm.mod)

## with xform of RUL -- transformed performs better (FD001)

lm.mod <- lm(RUL^.6 ~ ., train)
summary(lm.mod)
```

```{r Assumptions and Outliers}

## Lots of big outliers, should I remove these?  Should investigate

#influencePlot(lm.mod)
par(mfrow=c(2,2))
plot(lm.mod)
```

### Box-Cox Xform

```{r Q1e Box-Cox Procedure, eval=FALSE, include=FALSE}
## Neither Box-Cox works, must investigate

library(ALSM)
# bcsse <- boxcox.sse(train$sensor11, train$RUL, l=seq(-2,2,0.1))
# lambda <- bcsse$lambda[which.min(bcsse$SSE)]
# lambda
```

```{r Q4 Box-Cox Transformation, eval=FALSE, include=FALSE}
# library(MASS)
# bcmle <- boxcox(lm.mod, l=seq(-2,2,0.1))
# lambda <- bcmle$x[which.max(bcmle$y)]
# lambda
```

### Exploratory RF Model

```{r Initial RF Mod, eval=FALSE, include=FALSE}
start.time <- Sys.time()

exprf.mod <- randomForest(RUL ~ ., train)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

```


```{r Initial Random Forest Variable Importance Plot, eval=FALSE, include=FALSE}
var_imp <- varImpPlot(exprf.mod, main="Variable Importance Plot")
var_imp

#sensor11 and sensor4 have a high importance, which corresponds to their charts...as RUL decreases, we see a change in the sensor readings 
```

```{r}
if (is.element('sensor6', colnames(train)) == T){ 
  train <- subset(train, select = -c(sensor6))
}
```

```{r Datasave, echo=F}
#save.image(file='NASA_pm_EDA.RData')
```

### Classification

Add "op" column with following encoding:

    0  Operational
    1  Failure eminent (within "mxcycle" of failing -- if RUL < mxcycle, failure eminent)

```{r add "op" column}
mxcycle <- 60

train$op <- 0          ## add class column with all "operational"
train$op[train$RUL < mxcycle] <- 1   ## all rows with RUL < mxcycle are "Failure eminent"
train[train$op == 1,]    ## display all rows with "Failure eminent"
train <- subset(train, select = -c(RUL))   ## delete RUL column
df <- train
```


```{r Training and Test Sets}
## setting up training and test sets

set.seed(41)
trainrows <- sample(nrow(df), 0.7*nrow(df))

## non-onehot encoded sets
tr.df <- df[trainrows,]
te.df <- df[-trainrows,]

```

```{r}
cutoff <- .7   ## threshold for classification

## calculates accuracy for linear models
acc <- function(mod, newdata, response) {
  probs=predict(mod, newdata, type="response")
  pred=rep(0,nrow(newdata))
  pred[probs > cutoff] <- 1
  tab <- table(pred, response)
  accuracy <- (tab[1] + tab[4])/sum(tab)
  return(accuracy)
}

## calculates accuracy for nonlinear models
acc.tree <- function(mod, newdata, response) {
  probs=predict(mod, newdata)
  pred=rep(0,nrow(newdata))
  pred[probs > cutoff] <- 1
  tab <- table(pred, response)
  accuracy <- (tab[1] + tab[4])/sum(tab)
  return(accuracy)
}

## calculates accuracy for boost
accb.tree <- function(mod, newdata, response, ntree) {
  probs=predict(mod, newdata, ntree)
  pred=rep(0,nrow(newdata))
  pred[probs > cutoff] <- 1
  tab <- table(pred, response)
  accuracy <- (tab[1] + tab[4])/sum(tab)
  return(accuracy)
}

## returns table for linear models
tab <- function(mod, newdata, response) {
  probs=predict(mod, newdata, type="response")
  pred=rep(0,nrow(newdata))
  pred[probs > cutoff] <- 1
  tab <- table(pred, response)
  return(tab)
}

## returns table for tree models
tab.tree <- function(mod, newdata, response) {
  probs=predict(mod, newdata)
  pred=rep(0,nrow(newdata))
  pred[probs > cutoff] <- 1
  tab <- table(pred, response)
  return(tab)
}

f1 <- function(table){
  r <- table[1,1]/(table[1,1] + table[1,2])
  p <- table[1,1]/(table[1,1] + table[2,1])
  f1 <- 2*(r*p)/(r+p)
  return(f1)
}
```

### Null Model

Predicts that all classifications will be zero.  70.9% of the time the class is zero

```{r Null Model}
null.mod <- glm(op ~ 1, family=binomial, data=tr.df)  
summary(null.mod)

## all predictions will be 0

acc.null <- c(table(tr.df$op)[1]/nrow(tr.df), table(te.df$op)[1]/nrow(te.df))
acc.null

## Calculate number of percent of rows are zero
1-(sum(tr.df$op) / nrow(tr.df))

table(tr.df$op)
table(te.df$op)
```

### Linear Models

89 failure predictions when NOT failed

```{r Linear Model}
glm.mod <- glm(op ~., family=binomial, data=tr.df)

acc.lm <- c(acc(glm.mod, tr.df, tr.df$op), acc(glm.mod, te.df, te.df$op))
acc.lm

tab.lm <- tab(glm.mod, te.df, te.df$op)
tab.lm

# recall.lm <- tab.lm[1,1]/(tab.lm[1,1] + tab.lm[1,2])
# prec.lm <- tab.lm[1,1]/(tab.lm[1,1] + tab.lm[2,1])
# 
# f1.lm <- 2*(recall.lm*prec.lm)/(recall.lm + prec.lm)

f1(tab.lm)

```

```{r Model Assumptions, warning=FALSE, message=FALSE}
## A check of model assumptions
par(mfrow=c(2,2))
plot(glm.mod)
```

Should I include the LASSO?

  - LASSO has a low accuracy, but a low false positive rate

```{r LASSO Model}

## Setting up matrices for LASSO (using onehot df)
train_x <- as.matrix(subset(tr.df, select = -op))
train_y <- as.matrix(subset(tr.df, select = op))

test_x <- as.matrix(subset(te.df, select = -op))
test_y <- as.matrix(subset(te.df, select = op))

## Pick the best LASSO regression model using built-in K-fold CV
#set.seed(1)
cv_lasso <- cv.glmnet(train_x, train_y, alpha=1)


## Plot of MSE vs. lambda
plot(cv_lasso)

## Lambda with minimum MSE
cv_lasso$lambda.min

lasso_coefs <- coef(cv_lasso, s = "lambda.min")
length(lasso_coefs[lasso_coefs != 0])

lasso.mod <- glmnet(train_x, train_y, alpha=1, lambda=cv_lasso$lambda.min)        ## non-log version

acc.lasso <- c(acc(lasso.mod, train_x, train_y), acc(lasso.mod, test_x, test_y))
acc.lasso

tab.lasso <- tab(lasso.mod, test_x, test_y)
```

### GAM Placeholder

```{r Best Gam Model}
## Best GAM model from optimization
gam.mod <- gam(formula = op ~ s(sensor11, df = 5) + s(sensor12, df = 5) + s(sensor4, df=5) + s(sensor9, df=5) + s(sensor7, df=5) + s(sensor15, df=5) + s(sensor14, df=5) + s(sensor21, df=5) + s(sensor20, df=5) + s(sensor2, df=5), data = tr.df, trace = FALSE)

acc.gam <- c(acc(gam.mod, tr.df, tr.df$op), acc(gam.mod, te.df, te.df$op))
acc.gam

tab.gam <- tab(gam.mod, te.df, te.df$op)
tab.gam


```

### Random Forest

```{r Best Random Forest Model}
## default RF
rf.mod <- randomForest(op ~., data=tr.df)
acc.rf <- c(acc.tree(rf.mod, tr.df, tr.df$op),acc.tree(rf.mod, te.df, te.df$op))
acc.rf

tab.rf <- tab.tree(rf.mod, te.df, te.df$op)
tab.rf
```

```{r Random Forest Variable Importance Plot}
varImpPlot(rf.mod, main="Variable Importance Plot")
```

### Boost

```{r Best Boosting Model}
## Run gbm and get rmse with best hyperparameters
set.seed(1)
ntree <- 1000
boost.mod <- gbm(op ~., data=tr.df, distribution="gaussian",n.trees=ntree)

acc.boost <- c(accb.tree(boost.mod, tr.df, tr.df$op, ntree), accb.tree(boost.mod, te.df, te.df$op, ntree))
acc.boost
```

```{r RMSE for Best BART mod}
## BART setup
tr.df.Bart <- subset(tr.df, select = -c(op))
te.df.Bart <- subset(te.df, select = -c(op))

## Using hyperparameters from CV
#bart.mod <- bartMachine(X=tr.df.Bart, y=log(tr.df$op), num_trees=1000, k=3, q = .9, nu = 3, seed = 1)

## Using defaults (i.e. to skip CV)
bart.mod <- bartMachine(X=tr.df.Bart, y=tr.df$op, seed = 1)

acc.bart <- c(acc.tree(bart.mod, tr.df.Bart, tr.df$op),acc.tree(bart.mod, te.df.Bart, te.df$op)) 
acc.bart

#tab(bart.mod, te.df, te.df$op)
```

```{r Best SVR Model}
#svr.mod <- tune.out$best.model

svr.mod <- svm(op ~., data=tr.df, kernel="radial", cost=1, scale=FALSE)

summary(svr.mod)

acc.svr <- c(acc.tree(svr.mod, tr.df, tr.df$op), acc.tree(svr.mod, te.df, te.df$op))
acc.svr

tab.svr <- tab.tree(svr.mod, te.df, te.df$op)

tab.svr
```

### Mars (Earth)

```{r Earth Models}
## Earth model w/o pruning
#earth.mod <- earth(op ~., data=tr.df)

## Earth model w/ pruning
earth.mod <- earth(op ~., data=tr.df, pmethod="none")

acc.earth <- c(acc(earth.mod, tr.df, tr.df$op),acc(earth.mod, te.df, te.df$op))
acc.earth 

tab.earth <- tab.tree(earth.mod, te.df, te.df$op)
tab.earth
```

```{r Summary Table, include=TRUE}
## Makes df of error results
acc.df <- as.data.frame(rbind(acc.lm, acc.lasso, acc.rf, acc.boost, acc.earth, acc.bart, acc.svr, acc.null))
colnames(acc.df) <- c("Training RMSE", "Test RMSE")
acc.df[order(acc.df$`Test RMSE`),]


```

```{r F1 Table}
f1.df <- as.data.frame(rbind(f1(tab.lm), f1(tab.lasso), f1(tab.rf), f1(tab.earth), f1(tab.svr)))
colnames(f1.df) <-  "F1 Score"
f1.df$model <- c("lm", "lasso", "rf", "earth", "svr")
f1.df
```


##Discussion

Not surprisingly, the null model has the lowest error, as most of the time the units are running (classification = 0).  The random forest and linear models have the best F1 scores.  To truly decide which model is best, I'd need to better understand the operating conditions of the units.  For instance: 

a. What's the price of the repairs?  Removing the engines with before failure will increase the overall number of removals.
b. What's the cost of an inflight failure?  Are there backup engines?  If engine failure is near catastrophic, we'll need to ensure no failures go unnoticed.  

```{r Final Datasave, echo=F}
#save.image(file='NASA_pm_full.RData')
```
